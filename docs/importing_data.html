<!DOCTYPE html>

<html lang="en" data-content_root="./">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Importing Data &#8212; QuartzBio  documentation</title>
    <link rel="stylesheet" type="text/css" href="_static/pygments.css?v=5ecbeea2" />
    <link rel="stylesheet" type="text/css" href="_static/basic.css?v=b08954a9" />
    <link rel="stylesheet" type="text/css" href="_static/alabaster.css?v=27fed22d" />
    <script src="_static/documentation_options.js?v=5929fcd5"></script>
    <script src="_static/doctools.js?v=9bcbadda"></script>
    <script src="_static/sphinx_highlight.js?v=dc90522c"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Import Parameters" href="import_parameters.html" />
    <link rel="prev" title="Exporting Data" href="exporting_data.html" />
   
  <link rel="stylesheet" href="_static/custom.css" type="text/css" />
  

  
  

  </head><body>
  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          

          <div class="body" role="main">
            
  <section id="importing-data">
<h1>Importing Data<a class="headerlink" href="#importing-data" title="Link to this heading">¶</a></h1>
<section id="overview">
<h2>Overview<a class="headerlink" href="#overview" title="Link to this heading">¶</a></h2>
<p>The EDP specializes in harmonizing a variety of data sources through its robust import system. Importing data is the process of converting a flat file into a dataset that can be queried in real time. The EDP supports data in the most common formats, including JSONL, VCF, CSV, TSV, XML, GTF, and GFF3. Users can contact QuartzBio Support for assistance with importing many other formats (including custom, proprietary formats, and unstructured data).</p>
<p>The EDP’s import system automates the traditional ETL (Extract, Transform, Load) process. The process typically starts by uploading files into a vault. An import task can then be configured and launched. The import system automatically handles data extraction (file parsing), data transformation, data validation, and finally data loading. Users can refer to the <a class="reference external" href="https://quartzbio.github.io/quartzbio-python/import_parameters.html">Import Parameters documentation</a> for more information about configuring optional parameters for data parsing, entity detection, validation, and annotation.</p>
</section>
<section id="supported-formats">
<h2>Supported Formats<a class="headerlink" href="#supported-formats" title="Link to this heading">¶</a></h2>
<p>Users should note that only UTF-8 encoded files are supported. Content may be corrupted during the import process if non-UTF-8 files are imported.</p>
<p>The following file formats and extensions are supported:</p>
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>Name</p></th>
<th class="head"><p>File Extension</p></th>
<th class="head"><p>Previewable in EDP?</p></th>
<th class="head"><p>Transformable into a Dataset?</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Comma Separated Values</p></td>
<td><p>.csv</p></td>
<td><p>Y</p></td>
<td><p>Y</p></td>
</tr>
<tr class="row-odd"><td><p>General Feature Format</p></td>
<td><p>.gff3.gz</p></td>
<td><p>Y</p></td>
<td><p>Y</p></td>
</tr>
<tr class="row-even"><td><p>Gene Transfer Format</p></td>
<td><p>.gtf</p></td>
<td><p>Y</p></td>
<td><p>Y</p></td>
</tr>
<tr class="row-odd"><td><p>Hyper Text Markup Language</p></td>
<td><p>.html</p></td>
<td><p>Y</p></td>
<td><p>N</p></td>
</tr>
<tr class="row-even"><td><p>JavaScript Object Notation (in JSON Lines format)</p></td>
<td><p>.json</p></td>
<td><p>Y</p></td>
<td><p>Y</p></td>
</tr>
<tr class="row-odd"><td><p>Mutation Annotation Format</p></td>
<td><p>.maf</p></td>
<td><p>Y</p></td>
<td><p>Y</p></td>
</tr>
<tr class="row-even"><td><p>Portable Document Format</p></td>
<td><p>.pdf</p></td>
<td><p>Y</p></td>
<td><p>N</p></td>
</tr>
<tr class="row-odd"><td><p>Tab Separated Values</p></td>
<td><p>.tsv</p></td>
<td><p>Y</p></td>
<td><p>Y</p></td>
</tr>
<tr class="row-even"><td><p>Unformatted text file</p></td>
<td><p>.txt</p></td>
<td><p>Y</p></td>
<td><p>Y</p></td>
</tr>
<tr class="row-odd"><td><p>Variant Call Format</p></td>
<td><p>.vcf</p></td>
<td><p>Y</p></td>
<td><p>Y</p></td>
</tr>
<tr class="row-even"><td><p>Extensible Markup Language</p></td>
<td><p>.xml</p></td>
<td><p>Y</p></td>
<td><p>Y (requires  a template)</p></td>
</tr>
<tr class="row-odd"><td><p>Excel</p></td>
<td><p>.xlsx/.xsl</p></td>
<td><p>Y</p></td>
<td><p>Y</p></td>
</tr>
</tbody>
</table>
</section>
<section id="reader-parameters">
<h2>Reader Parameters<a class="headerlink" href="#reader-parameters" title="Link to this heading">¶</a></h2>
<p>EDP automatically detects the file format based on the extension, except for the Nirvana JSON file, and parses the file using a specialized <code class="docutils literal notranslate"><span class="pre">&quot;reader&quot;</span></code>. It is possible to manually specify a reader and modify reader parameters using the <a class="reference external" href="https://quartzbio.github.io/quartzbio-python/import_parameters.html">reader_params</a> attribute of the DatasetImport resource.</p>
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>Reader</p></th>
<th class="head"><p>Reader name</p></th>
<th class="head"><p>Extension</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>VCF</p></td>
<td><p>vcf</p></td>
<td><p>.vcf</p></td>
</tr>
<tr class="row-odd"><td><p>JSONL</p></td>
<td><p>json</p></td>
<td><p>.json</p></td>
</tr>
<tr class="row-even"><td><p>CSV</p></td>
<td><p>csv</p></td>
<td><p>.csv</p></td>
</tr>
<tr class="row-odd"><td><p>TSV</p></td>
<td><p>tsv</p></td>
<td><p>.tsv, .txt, .maf</p></td>
</tr>
<tr class="row-even"><td><p>XML</p></td>
<td><p>XML</p></td>
<td><p>.xml</p></td>
</tr>
<tr class="row-odd"><td><p>GTF</p></td>
<td><p>gft</p></td>
<td><p>.gtf</p></td>
</tr>
<tr class="row-even"><td><p>GFF3</p></td>
<td><p>gff3</p></td>
<td><p>.gff3</p></td>
</tr>
<tr class="row-odd"><td><p>Nirvana</p></td>
<td><p>json</p></td>
<td><p>nirvana .json</p></td>
</tr>
<tr class="row-even"><td><p>Excel</p></td>
<td><p>xlsx</p></td>
<td><p>.xlsx</p></td>
</tr>
<tr class="row-odd"><td><p>Excel</p></td>
<td><p>xls</p></td>
<td><p>.xls</p></td>
</tr>
</tbody>
</table>
<p>EDP supports GZip compression for all file types. Gzipped files must have the .gz file extension in addition to their format extension (i.e. file.vcf.gz). Users are recommended to compress files with GZip for faster uploads and imports.</p>
</section>
<section id="importing-from-files">
<h2>Importing from Files<a class="headerlink" href="#importing-from-files" title="Link to this heading">¶</a></h2>
<p>The first step to getting data onto EDP is by uploading files into a vault. Users can refer to the <a class="reference external" href="https://quartzbio.github.io/quartzbio-python/vaults_and_objects.html">Vaults documentation</a> for more information.</p>
<p>In Python:</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">quartzbio</span><span class="w"> </span><span class="kn">import</span> <span class="n">Vault</span><span class="p">,</span> <span class="n">Object</span>
<span class="c1"># Upload a local file to the root of the personal Vault</span>
<span class="n">vault</span> <span class="o">=</span> <span class="n">Vault</span><span class="o">.</span><span class="n">get_personal_vault</span><span class="p">()</span>
<span class="n">uploaded_file</span> <span class="o">=</span> <span class="n">vault</span><span class="o">.</span><span class="n">upload_file</span><span class="p">(</span><span class="s1">&#39;local/path/file.vcf.gz&#39;</span><span class="p">,</span> <span class="s1">&#39;/&#39;</span><span class="p">)</span>
<span class="c1"># Retrieve the file by its full path:</span>
<span class="n">uploaded_file</span> <span class="o">=</span> <span class="n">Object</span><span class="o">.</span><span class="n">get_by_full_path</span><span class="p">(</span><span class="s1">&#39;~/file.vcf.gz&#39;</span><span class="p">)</span>
</pre></div>
</div>
<p>Once the files have been uploaded, they can be imported into any new or existing dataset (<a class="reference external" href="https://quartzbio.github.io/quartzbio-python/creating_and_migrating_datasets.html#creating-datasets">Learn how to create a dataset</a>). To launch an import, users can utilize the DatasetImport method. The user will need to provide the uploaded file and target dataset as inputs. Once the import has been launched, it is possible to track the progress through the API on the web interface through the Activity tab.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">quartzbio</span><span class="w"> </span><span class="kn">import</span> <span class="n">Dataset</span><span class="p">,</span> <span class="n">DatasetImport</span>

<span class="n">dataset</span> <span class="o">=</span> <span class="n">Dataset</span><span class="o">.</span><span class="n">get_or_create_by_full_path</span><span class="p">(</span><span class="s1">&#39;~/python_examples/test_dataset&#39;</span><span class="p">)</span>

<span class="c1"># Launch the import</span>
<span class="n">imp</span> <span class="o">=</span> <span class="n">DatasetImport</span><span class="o">.</span><span class="n">create</span><span class="p">(</span>
    <span class="n">dataset_id</span><span class="o">=</span><span class="n">dataset</span><span class="o">.</span><span class="n">id</span><span class="p">,</span>
    <span class="n">object_id</span><span class="o">=</span><span class="n">uploaded_file</span><span class="o">.</span><span class="n">id</span><span class="p">,</span>
    <span class="n">commit_mode</span><span class="o">=</span><span class="s1">&#39;append&#39;</span>
<span class="p">)</span>

<span class="c1"># Follow the import status</span>
<span class="n">dataset</span><span class="o">.</span><span class="n">activity</span><span class="p">(</span><span class="n">follow</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="importing-from-urls">
<h2>Importing from URLs<a class="headerlink" href="#importing-from-urls" title="Link to this heading">¶</a></h2>
<p>If the files are on a remote server and accessible by URL, they can be imported using a manifest. <code class="docutils literal notranslate"><span class="pre">A</span> <span class="pre">manifest</span></code> is simply a list of files (URLs and other attributes) to import:</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">quartzbio</span><span class="w"> </span><span class="kn">import</span> <span class="n">Manifest</span>

<span class="n">source_url</span> <span class="o">=</span> <span class="s2">&quot;https://s3.amazonaws.com/downloads.quartzbio.com/demo/interesting-variants.json.gz&quot;</span>

<span class="n">manifest</span> <span class="o">=</span> <span class="n">Manifest</span><span class="p">()</span>
<span class="n">manifest</span><span class="o">.</span><span class="n">add_url</span><span class="p">(</span><span class="n">source_url</span><span class="p">)</span>
</pre></div>
</div>
<p>Once the manifest has been created, it can be imported into any new or existing dataset. To launch an import, users can employ the DatasetImport resource, providing the manifest and target dataset as input. Once the import has been launched it is available to track the progress through the API or on the web.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">quartzbio</span><span class="w"> </span><span class="kn">import</span> <span class="n">Dataset</span><span class="p">,</span> <span class="n">DatasetImport</span>

<span class="n">dataset</span> <span class="o">=</span> <span class="n">Dataset</span><span class="o">.</span><span class="n">get_or_create_by_full_path</span><span class="p">(</span><span class="s1">&#39;~/python_examples/manifest_dataset&#39;</span><span class="p">)</span>

<span class="c1"># Launch the import</span>
<span class="n">imp</span> <span class="o">=</span> <span class="n">DatasetImport</span><span class="o">.</span><span class="n">create</span><span class="p">(</span>
    <span class="n">dataset_id</span><span class="o">=</span><span class="n">dataset</span><span class="o">.</span><span class="n">id</span><span class="p">,</span>
    <span class="n">manifest</span><span class="o">=</span><span class="n">manifest</span><span class="o">.</span><span class="n">manifest</span><span class="p">,</span>
    <span class="n">commit_mode</span><span class="o">=</span><span class="s1">&#39;append&#39;</span>
<span class="p">)</span>

<span class="c1"># Follow the import status</span>
<span class="n">dataset</span><span class="o">.</span><span class="n">activity</span><span class="p">(</span><span class="n">follow</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
<p>The EDP can also pull data from many other pipelines. Users can contact QuartzBio Support for more information.</p>
</section>
<section id="importing-from-records">
<h2>Importing from Records<a class="headerlink" href="#importing-from-records" title="Link to this heading">¶</a></h2>
<p>The EDP can also import data as a list of records, i.e. a list of Python dictionaries or R data. Users should note that the EDP supports only importing up to <code class="docutils literal notranslate"><span class="pre">5000</span> <span class="pre">records</span></code> at a time through this method. Importing from records is most optimal for importing small datasets and making edits to datasets. For larger imports and transforms, users are recommended to import from compressed JSONL files.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">quartzbio</span><span class="w"> </span><span class="kn">import</span> <span class="n">Dataset</span><span class="p">,</span> <span class="n">DatasetImport</span>

<span class="n">records</span> <span class="o">=</span> <span class="p">[</span>
    <span class="p">{</span><span class="s1">&#39;gene&#39;</span><span class="p">:</span> <span class="s1">&#39;CFTR&#39;</span><span class="p">,</span> <span class="s1">&#39;importance&#39;</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span> <span class="s1">&#39;sample_count&#39;</span><span class="p">:</span> <span class="mi">2104</span><span class="p">},</span>
    <span class="p">{</span><span class="s1">&#39;gene&#39;</span><span class="p">:</span> <span class="s1">&#39;BRCA1&#39;</span><span class="p">,</span> <span class="s1">&#39;importance&#39;</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span> <span class="s1">&#39;sample_count&#39;</span><span class="p">:</span> <span class="mi">1391</span><span class="p">},</span>
    <span class="p">{</span><span class="s1">&#39;gene&#39;</span><span class="p">:</span> <span class="s1">&#39;CLIC2&#39;</span><span class="p">,</span> <span class="s1">&#39;importance&#39;</span><span class="p">:</span> <span class="mi">5</span><span class="p">,</span> <span class="s1">&#39;sample_count&#39;</span><span class="p">:</span> <span class="mi">14</span><span class="p">},</span>
<span class="p">]</span>
<span class="n">dataset</span> <span class="o">=</span> <span class="n">Dataset</span><span class="o">.</span><span class="n">get_or_create_by_full_path</span><span class="p">(</span><span class="s1">&#39;~/python_examples/records_dataset&#39;</span><span class="p">)</span>
<span class="n">imp</span> <span class="o">=</span> <span class="n">DatasetImport</span><span class="o">.</span><span class="n">create</span><span class="p">(</span>
    <span class="n">dataset_id</span><span class="o">=</span><span class="n">dataset</span><span class="o">.</span><span class="n">id</span><span class="p">,</span>
    <span class="n">data_records</span><span class="o">=</span><span class="n">records</span>
<span class="p">)</span>
</pre></div>
</div>
</section>
<section id="command-line-tools">
<h2>Command Line Tools<a class="headerlink" href="#command-line-tools" title="Link to this heading">¶</a></h2>
<p>Users can import data files using the quartzbio <strong>import</strong> command from the quartzbio Python module:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># Import a file (create the dataset if necessary):</span>
<span class="n">quartzbio</span> <span class="kn">import</span><span class="w"> </span><span class="o">--</span><span class="n">create</span><span class="o">-</span><span class="n">dataset</span> <span class="o">--</span><span class="n">follow</span> <span class="o">~/</span><span class="n">test</span><span class="o">-</span><span class="n">dataset</span> <span class="n">data</span><span class="o">.</span><span class="n">vcf</span><span class="o">.</span><span class="n">gz</span>

<span class="c1"># Import files in upsert mode (create the dataset from a template if necessary):</span>
<span class="n">quartzbio</span> <span class="kn">import</span><span class="w"> </span><span class="o">--</span><span class="n">create</span><span class="o">-</span><span class="n">dataset</span> <span class="o">--</span><span class="n">template</span><span class="o">-</span><span class="n">file</span> <span class="n">template</span><span class="o">.</span><span class="n">json</span> <span class="o">--</span><span class="n">commit</span><span class="o">-</span><span class="n">mode</span><span class="o">=</span><span class="n">upsert</span> <span class="o">--</span><span class="n">follow</span> <span class="o">~/</span><span class="n">test</span><span class="o">-</span><span class="n">dataset</span> <span class="n">data</span><span class="o">.</span><span class="n">vcf</span><span class="o">.</span><span class="n">gz</span>
</pre></div>
</div>
<p>Users can create a dataset template file (e.g. template.json) to be used during file import:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="p">{</span>
  <span class="s2">&quot;name&quot;</span><span class="p">:</span> <span class="s2">&quot;Example EDP Dataset template&quot;</span><span class="p">,</span>
  <span class="s2">&quot;fields&quot;</span><span class="p">:</span> <span class="p">[</span>
    <span class="p">{</span>
      <span class="s2">&quot;name&quot;</span><span class="p">:</span> <span class="s2">&quot;gene_symbol&quot;</span><span class="p">,</span>
      <span class="s2">&quot;description&quot;</span><span class="p">:</span> <span class="s2">&quot;HUGO gene symbol&quot;</span><span class="p">,</span>
      <span class="s2">&quot;entity_type&quot;</span><span class="p">:</span> <span class="s2">&quot;gene&quot;</span><span class="p">,</span>
      <span class="s2">&quot;data_type&quot;</span><span class="p">:</span> <span class="s2">&quot;string&quot;</span>
    <span class="p">},</span>
    <span class="p">{</span>
      <span class="s2">&quot;name&quot;</span><span class="p">:</span> <span class="s2">&quot;variant&quot;</span><span class="p">,</span>
      <span class="s2">&quot;entity_type&quot;</span><span class="p">:</span> <span class="s2">&quot;variant&quot;</span><span class="p">,</span>
      <span class="s2">&quot;data_type&quot;</span><span class="p">:</span> <span class="s2">&quot;string&quot;</span>
    <span class="p">}</span>
  <span class="p">]</span>
<span class="p">}</span>
</pre></div>
</div>
<p>An easy way to upload files to a vault in batches is to use the quartzbio upload command:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># Upload local_folder to the root of the personal vault:</span>
<span class="n">quartzbio</span> <span class="n">upload</span> <span class="o">./</span><span class="n">local_folder</span>
</pre></div>
</div>
<p>If multiple files are uploaded, the command will cross-check the files and upload only the missing files and folders. Users should note that comparison is performed by <strong>filename, not by file contents</strong>. Users can refer to the <a class="reference external" href="https://quartzbio.github.io/quartzbio-python/vaults_and_objects.html">Vaults and Objects</a> documentation for more information about batch uploading.</p>
</section>
<section id="transforming-imported-data">
<h2>Transforming Imported Data<a class="headerlink" href="#transforming-imported-data" title="Link to this heading">¶</a></h2>
<p>Imported data can be transformed (fields added or edited) by providing a list of fields to the <code class="docutils literal notranslate"><span class="pre">target_fields</span></code> parameter. <a class="reference external" href="https://quartzbio.github.io/quartzbio-python/expressions.html">Expressions</a> can be used to dynamically modify the data as it is imported, making it possible to</p>
<ul class="simple">
<li><p>Modify data types (numbers to strings or vice-versa)</p></li>
<li><p>Add new fields with static or dynamic content</p></li>
<li><p>Format strings and dates to clean the data</p></li>
<li><p>Merge data from datasets</p></li>
</ul>
<p>The following example imports a list of records and transforms the contents in a single step:</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">quartzbio</span><span class="w"> </span><span class="kn">import</span> <span class="n">Dataset</span><span class="p">,</span> <span class="n">DatasetImport</span>

<span class="n">dataset</span> <span class="o">=</span> <span class="n">Dataset</span><span class="o">.</span><span class="n">get_or_create_by_full_path</span><span class="p">(</span><span class="s1">&#39;~/python_examples/transform_import&#39;</span><span class="p">)</span>

<span class="c1"># The original records</span>
<span class="n">records</span> <span class="o">=</span> <span class="p">[</span>
    <span class="p">{</span><span class="s1">&#39;name&#39;</span><span class="p">:</span> <span class="s1">&#39;Francis Crick&#39;</span><span class="p">},</span>
    <span class="p">{</span><span class="s1">&#39;name&#39;</span><span class="p">:</span> <span class="s1">&#39;James Watson&#39;</span><span class="p">},</span>
    <span class="p">{</span><span class="s1">&#39;name&#39;</span><span class="p">:</span> <span class="s1">&#39;Rosalind Franklin&#39;</span><span class="p">}</span>
<span class="p">]</span>

<span class="c1"># The transforms to apply through &quot;target_fields&quot;</span>
<span class="c1"># Compute the first and last names.</span>
<span class="n">target_fields</span> <span class="o">=</span> <span class="p">[</span>
    <span class="p">{</span>
        <span class="s2">&quot;name&quot;</span><span class="p">:</span> <span class="s2">&quot;first_name&quot;</span><span class="p">,</span>
        <span class="s2">&quot;description&quot;</span><span class="p">:</span> <span class="s2">&quot;Adds a first name column based on name column&quot;</span><span class="p">,</span>
        <span class="s2">&quot;data_type&quot;</span><span class="p">:</span> <span class="s2">&quot;string&quot;</span><span class="p">,</span>
        <span class="s2">&quot;expression&quot;</span><span class="p">:</span> <span class="s2">&quot;record.name.split(&#39; &#39;)[0]&quot;</span>
    <span class="p">},</span>
    <span class="p">{</span>
        <span class="s2">&quot;name&quot;</span><span class="p">:</span> <span class="s2">&quot;last_name&quot;</span><span class="p">,</span>
        <span class="s2">&quot;description&quot;</span><span class="p">:</span> <span class="s2">&quot;Adds a last name column based on name column&quot;</span><span class="p">,</span>
        <span class="s2">&quot;data_type&quot;</span><span class="p">:</span> <span class="s2">&quot;string&quot;</span><span class="p">,</span>
        <span class="s2">&quot;expression&quot;</span><span class="p">:</span> <span class="s2">&quot;record.name.split(&#39; &#39;)[-1]&quot;</span>
    <span class="p">}</span>
<span class="p">]</span>

<span class="n">imp</span> <span class="o">=</span> <span class="n">DatasetImport</span><span class="o">.</span><span class="n">create</span><span class="p">(</span>
    <span class="n">dataset_id</span><span class="o">=</span><span class="n">dataset</span><span class="o">.</span><span class="n">id</span><span class="p">,</span>
    <span class="n">data_records</span><span class="o">=</span><span class="n">records</span><span class="p">,</span>
    <span class="n">target_fields</span><span class="o">=</span><span class="n">target_fields</span>
<span class="p">)</span>

<span class="c1"># Wait until the import finishes</span>
<span class="n">dataset</span><span class="o">.</span><span class="n">activity</span><span class="p">(</span><span class="n">follow</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="k">for</span> <span class="n">record</span> <span class="ow">in</span> <span class="n">dataset</span><span class="o">.</span><span class="n">query</span><span class="p">(</span><span class="n">exclude_fields</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;_id&#39;</span><span class="p">,</span> <span class="s1">&#39;_commit&#39;</span><span class="p">]):</span>
    <span class="nb">print</span> <span class="n">record</span>

<span class="c1"># Output:</span>
<span class="c1"># {&#39;first_name&#39;: &#39;Francis&#39;, &#39;last_name&#39;: &#39;Crick&#39;, &#39;name&#39;: &#39;Francis Crick&#39;}</span>
<span class="c1"># {&#39;first_name&#39;: &#39;James&#39;, &#39;last_name&#39;: &#39;Watson&#39;, &#39;name&#39;: &#39;James Watson&#39;}</span>
<span class="c1"># {&#39;first_name&#39;: &#39;Rosalind&#39;, &#39;last_name&#39;: &#39;Franklin&#39;, &#39;name&#39;: &#39;Rosalind Franklin&#39;}</span>
</pre></div>
</div>
<p>Existing imported data can also be modified by using migrations. This allows a user to add a column, modify data within a column, or remove a column.</p>
</section>
<section id="validating-imported-data">
<h2>Validating Imported Data<a class="headerlink" href="#validating-imported-data" title="Link to this heading">¶</a></h2>
<p>When importing data, every record is validated to ensure it can be committed into a <code class="docutils literal notranslate"><span class="pre">Dataset</span></code>. Validation compares the schema of existing Dataset fields with the values of incoming data and issues validation errors if the Dataset field schema does not match the incoming value. Validation can also issue warnings.</p>
<p>During validation, a field’s <code class="docutils literal notranslate"><span class="pre">data_type</span></code> and <code class="docutils literal notranslate"><span class="pre">is_list</span></code> values are checked. All records are evaluated (although users may override this to fail fast on the first error). A commit will not be created if there are any validation errors.</p>
<p>The following settings can be passed to the <a class="reference external" href="https://quartzbio.github.io/quartzbio-python/import_parameters.html#entity-detection-parameters">validation_params</a> field.</p>
<ul class="simple">
<li><p>disable - (boolean) default False - Disables validation completely</p></li>
<li><p>raise_on_errors - (boolean) default False - Will fail the import on first validation error encountered.</p></li>
<li><p>strict_validation - (boolean) default False - Will upgrade all validation warnings to errors.</p></li>
<li><p>allow_new_fields - (boolean) default False - If strict validation is True, will still allow new fields to be added</p></li>
</ul>
<p>The following example fails an import as soon as invalid data is detected:</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="n">imp</span> <span class="o">=</span> <span class="n">DatasetImport</span><span class="o">.</span><span class="n">create</span><span class="p">(</span>
    <span class="n">dataset_id</span><span class="o">=</span><span class="n">dataset</span><span class="o">.</span><span class="n">id</span><span class="p">,</span>
    <span class="n">object_id</span><span class="o">=</span><span class="nb">object</span><span class="o">.</span><span class="n">id</span><span class="p">,</span>
    <span class="n">validation_params</span><span class="o">=</span><span class="p">{</span>
        <span class="s1">&#39;raise_on_errors&#39;</span><span class="p">:</span> <span class="kc">True</span>
    <span class="p">}</span>
<span class="p">)</span>
</pre></div>
</div>
<p>The following example disables validation from running, which can improve import performance.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="n">imp</span> <span class="o">=</span> <span class="n">DatasetImport</span><span class="o">.</span><span class="n">create</span><span class="p">(</span>
    <span class="n">dataset_id</span><span class="o">=</span><span class="n">dataset</span><span class="o">.</span><span class="n">id</span><span class="p">,</span>
    <span class="n">object_id</span><span class="o">=</span><span class="nb">object</span><span class="o">.</span><span class="n">id</span><span class="p">,</span>
    <span class="n">validation_params</span><span class="o">=</span><span class="p">{</span>
        <span class="s1">&#39;disable&#39;</span><span class="p">:</span> <span class="kc">True</span>
    <span class="p">}</span>
<span class="p">)</span>
</pre></div>
</div>
</section>
<section id="committing-imported-data">
<h2>Committing Imported Data<a class="headerlink" href="#committing-imported-data" title="Link to this heading">¶</a></h2>
<p>Once data has been extracted from files, transformed, and validated, it will be automatically indexed (“committed”) into EDP’s datastore. Dataset commits represent all changes made to the target dataset by the import process. Four commit modes can be selected depending on the scenario: append (default), overwrite, upsert, and delete. The commit mode can be specified when creating the DatasetImport using the <code class="docutils literal notranslate"><span class="pre">commit_mode</span></code> parameter.</p>
<p><strong>append (default)</strong></p>
<p>Append mode always adds records to the dataset. Imported record IDs (the _id field) will be overwritten with unique values. <strong>Only append commits can be rolled back at this time.</strong></p>
<p><strong>overwrite</strong></p>
<p>Overwrite mode requires that each record have a value in the _id field. Existing records with the same _id are overwritten completely.</p>
<p><strong>upsert</strong></p>
<p>Upsert mode merges imported records with existing records, based on the value of their _id field. Object fields are merged, scalar fields (such as integers and strings) are overwritten, and new fields are added. <strong>List fields are completely overwritten regardless of the data type.</strong></p>
<p><strong>delete</strong></p>
<p>Delete mode is a special case that deletes existing dataset records based on their _id field.</p>
</section>
<section id="performance-tips">
<h2>Performance Tips<a class="headerlink" href="#performance-tips" title="Link to this heading">¶</a></h2>
<p>Below are some tips for improving the performance of dataset imports.</p>
<p><strong>Disable Data validation</strong></p>
<p>Data validation is enabled by default when running imports or migrations. This is used for data type checking on each record that is processed. Disabling this will provide a per-record performance improvement, translating to substantial time savings for large datasets.</p>
<p><strong>Dataset Capacity</strong></p>
<p>For many simultaneous imports, use a larger dataset capacity. Simultaneous imports have a high upper limit (50+) but simultaneous commits are throttled. Every import spawns a commit that does the actual indexing of the data. small capacity datasets allow a single running commit per dataset at a time, the medium allows <code class="docutils literal notranslate"><span class="pre">2</span> <span class="pre">simultaneous</span> <span class="pre">commits</span></code>, and large allows <code class="docutils literal notranslate"><span class="pre">3</span> <span class="pre">simultaneous</span> <span class="pre">commits</span></code>. Commits will remain queued until running ones are completed.</p>
<p>Indexing operations and query operations are also faster for larger-capacity datasets. If it is expected a dataset to be queried at high frequency, then we recommend using a larger dataset. If the dataset already exists, copy the dataset into a medium or large dataset.</p>
<p><strong>Optimize “expensive” Expressions</strong></p>
<p>Some dataset field expressions are more expensive than others. Dataset query expressions can be sped up by applying exact filters, using fields to only pull back the fields that are needed, or using <code class="docutils literal notranslate"><span class="pre">dataset_count()</span></code> if the length is what is needed.</p>
</section>
<section id="api-endpoints">
<h2>API Endpoints<a class="headerlink" href="#api-endpoints" title="Link to this heading">¶</a></h2>
<p>Methods do not accept URL parameters or request bodies unless specified. Please note that if your EDP endpoint is sponsor-cloud.edp.aws.quartz.bio, you would use sponsor-cloud.api.edp.aws.quartz.bio.
For correct work of the API, you need to change <code class="docutils literal notranslate"><span class="pre">&lt;EDP_API_HOST&gt;</span></code> to your current domain, such as my-domain.api.edp.aws.quartz.bio</p>
<section id="dataset-imports">
<h3>Dataset Imports<a class="headerlink" href="#dataset-imports" title="Link to this heading">¶</a></h3>
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>Method</p></th>
<th class="head"><p>HTTP Request</p></th>
<th class="head"><p>Description</p></th>
<th class="head"><p>Authorization</p></th>
<th class="head"><p>Response</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>create</p></td>
<td><p>POST <a class="reference external" href="https:/">https:/</a>/&lt;EDP_API_HOST&gt;/v2/dataset_imports</p></td>
<td><p>Create a dataset import for a dataset.</p></td>
<td><p>This request requires an authorized user with write permission on the dataset.</p></td>
<td><p>The response returns “HTTP 201 Created”, along with the DatasetImport resource when successful.</p></td>
</tr>
</tbody>
</table>
<p>Request Body:</p>
<p>In the request body, provide an object with the following properties:</p>
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>Property</p></th>
<th class="head"><p>Value</p></th>
<th class="head"><p>Description</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>commit_mode</p></td>
<td><p>string</p></td>
<td><p>A valid commit mode.</p></td>
</tr>
<tr class="row-odd"><td><p>dataset_id</p></td>
<td><p>integer</p></td>
<td><p>(Optional) The ID of an existing object on EDP.</p></td>
</tr>
<tr class="row-even"><td><p>object_id</p></td>
<td><p>integer</p></td>
<td><p>(Optional) A file manifest (see below).</p></td>
</tr>
<tr class="row-odd"><td><p>manifest</p></td>
<td><p>object</p></td>
<td><p>(Optional) A vault location to store the export output (must be an EDP full path).</p></td>
</tr>
<tr class="row-even"><td><p>data_records</p></td>
<td><p>objects</p></td>
<td><p>(Optional) A list of records to import synchronously.</p></td>
</tr>
<tr class="row-odd"><td><p>description</p></td>
<td><p>string</p></td>
<td><p>(Optional) A description of this import.</p></td>
</tr>
<tr class="row-even"><td><p>entity_params</p></td>
<td><p>object</p></td>
<td><p>(Optional) Configuration parameters for entity detection.</p></td>
</tr>
<tr class="row-odd"><td><p>reader_params</p></td>
<td><p>object</p></td>
<td><p>(Optional) Configuration parameters for readers.</p></td>
</tr>
<tr class="row-even"><td><p>validation_params</p></td>
<td><p>object</p></td>
<td><p>(Optional) Configuration parameters for validation.</p></td>
</tr>
<tr class="row-odd"><td><p>annotator_params</p></td>
<td><p>object</p></td>
<td><p>(Optional) Configuration parameters for the Annotator.</p></td>
</tr>
<tr class="row-even"><td><p>include_errors</p></td>
<td><p>boolean</p></td>
<td><p>If True, a new field (_errors) will be added to each record containing expression evaluation errors (default: True).</p></td>
</tr>
<tr class="row-odd"><td><p>target_fields</p></td>
<td><p>objects</p></td>
<td><p>A list of valid dataset fields to create or override in the import.</p></td>
</tr>
<tr class="row-even"><td><p>priority</p></td>
<td><p>integer</p></td>
<td><p>A priority to assign to this task</p></td>
</tr>
</tbody>
</table>
<p>When creating a new import, either manifest, <code class="docutils literal notranslate"><span class="pre">object_id</span></code> or <code class="docutils literal notranslate"><span class="pre">data_records</span></code> must be provided. Using a manifest allows you to import a remote file accessible by HTTP(S), for example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># Example Manifest</span>
<span class="p">{</span>
    <span class="s2">&quot;files&quot;</span><span class="p">:</span> <span class="p">[{</span>
        <span class="s2">&quot;url&quot;</span><span class="p">:</span> <span class="s2">&quot;https://example.com/file.json.gz&quot;</span><span class="p">,</span>
        <span class="s2">&quot;name&quot;</span><span class="p">:</span> <span class="s2">&quot;file.json.gz&quot;</span><span class="p">,</span>
        <span class="s2">&quot;format&quot;</span><span class="p">:</span> <span class="s2">&quot;json&quot;</span><span class="p">,</span>
        <span class="s2">&quot;size&quot;</span><span class="p">:</span> <span class="mi">100</span><span class="p">,</span>
        <span class="s2">&quot;md5&quot;</span><span class="p">:</span> <span class="s2">&quot;&quot;</span><span class="p">,</span>
        <span class="s2">&quot;base64_md5&quot;</span><span class="p">:</span> <span class="s2">&quot;&quot;</span>
    <span class="p">}]</span>
<span class="p">}</span>
</pre></div>
</div>
<p>Manifests can include the following parameters:</p>
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>Property</p></th>
<th class="head"><p>Value</p></th>
<th class="head"><p>Description</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>url</p></td>
<td><p>string</p></td>
<td><p>A publicly accessible URL pointing to a file to import into EDP. You must pass a URL or an object_id.</p></td>
</tr>
<tr class="row-odd"><td><p>object_id</p></td>
<td><p>long</p></td>
<td><p>The ID of an existing object on EDP. You must pass an object_id or a URL.</p></td>
</tr>
<tr class="row-even"><td><p>name</p></td>
<td><p>string</p></td>
<td><p>(Optional) The name of the file. If not passed, EDP will take it from the URL or object.</p></td>
</tr>
<tr class="row-odd"><td><p>format</p></td>
<td><p>string</p></td>
<td><p>(Optional) The file format of the file. If not passed, EDP will take it from the URL or object.</p></td>
</tr>
<tr class="row-even"><td><p>md5</p></td>
<td><p>string</p></td>
<td><p>(Optional) The md5 hash of the file contents. If passed, EDP will validate the file after downloading and fail if mismatched.</p></td>
</tr>
<tr class="row-odd"><td><p>entity_params</p></td>
<td><p>object</p></td>
<td><p>(Optional) Configuration parameters for entity detection.</p></td>
</tr>
<tr class="row-even"><td><p>reader_params</p></td>
<td><p>object</p></td>
<td><p>(Optional) Configuration parameters for readers.</p></td>
</tr>
<tr class="row-odd"><td><p>validation_params</p></td>
<td><p>object</p></td>
<td><p>(Optional)  Configuration parameters for validation.</p></td>
</tr>
</tbody>
</table>
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>Method</p></th>
<th class="head"><p>HTTP Request</p></th>
<th class="head"><p>Description</p></th>
<th class="head"><p>Authorization</p></th>
<th class="head"><p>Response</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>delete</p></td>
<td><p>DELETE <a class="reference external" href="https:/">https:/</a>/&lt;EDP_API_HOST&gt;/v2/dataset_imports/{ID}</p></td>
<td><p>Delete a dataset import.</p></td>
<td><p>This request requires an authorized user with write permission on the dataset.</p></td>
<td><p>The response returns “HTTP 200 OK” when successful.</p></td>
</tr>
</tbody>
</table>
<p><strong>Deleting dataset imports is not recommended as data provenance will be lost.</strong></p>
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>Method</p></th>
<th class="head"><p>HTTP Request</p></th>
<th class="head"><p>Description</p></th>
<th class="head"><p>Authorization</p></th>
<th class="head"><p>Response</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>get</p></td>
<td><p>GET <a class="reference external" href="https:/">https:/</a>/&lt;EDP_API_HOST&gt;/v2/dataset_imports/{ID}</p></td>
<td><p>Retrieve metadata about an import.</p></td>
<td><p>This request requires an authorized user with read permission on the dataset.</p></td>
<td><p>The response contains a DatasetImport resource.</p></td>
</tr>
<tr class="row-odd"><td><p>list</p></td>
<td><p>GET <a class="reference external" href="https:/">https:/</a>/&lt;EDP_API_HOST&gt;/v2/datasets/{DATASET_ID}/imports</p></td>
<td><p>List the imports associated with a dataset.</p></td>
<td><p>This request requires an authorized user with read permission on the dataset.</p></td>
<td><p>The response contains a list of DatasetImport resources.</p></td>
</tr>
<tr class="row-even"><td><p>cancel</p></td>
<td><p>PUT <a class="reference external" href="https:/">https:/</a>/&lt;EDP_API_HOST&gt;/v2/dataset_imports/{IMPORT_ID}</p></td>
<td><p>Cancel a dataset import.</p></td>
<td><p>This request requires an authorized user with write permission on the dataset.</p></td>
<td><p>The response will contain a DatasetImport resource with the status canceled.</p></td>
</tr>
</tbody>
</table>
<p>Request Body</p>
<p>In the request body, provide a valid DatasetResource object (see <em>create</em> above) with status = canceled.</p>
</section>
</section>
</section>


          </div>
          
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="Main">
        <div class="sphinxsidebarwrapper">
<h1 class="logo"><a href="index.html">QuartzBio</a></h1>









<search id="searchbox" style="display: none" role="search">
    <div class="searchformwrapper">
    <form class="search" action="search.html" method="get">
      <input type="text" name="q" aria-labelledby="searchlabel" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" placeholder="Search"/>
      <input type="submit" value="Go" />
    </form>
    </div>
</search>
<script>document.getElementById('searchbox').style.display = "block"</script><h3>Navigation</h3>
<p class="caption" role="heading"><span class="caption-text">Authentication</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="authentication.html">Authentication</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Vaults and Files</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="exporting_data.html">Exporting Data</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Importing Data</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#overview">Overview</a></li>
<li class="toctree-l2"><a class="reference internal" href="#supported-formats">Supported Formats</a></li>
<li class="toctree-l2"><a class="reference internal" href="#reader-parameters">Reader Parameters</a></li>
<li class="toctree-l2"><a class="reference internal" href="#importing-from-files">Importing from Files</a></li>
<li class="toctree-l2"><a class="reference internal" href="#importing-from-urls">Importing from URLs</a></li>
<li class="toctree-l2"><a class="reference internal" href="#importing-from-records">Importing from Records</a></li>
<li class="toctree-l2"><a class="reference internal" href="#command-line-tools">Command Line Tools</a></li>
<li class="toctree-l2"><a class="reference internal" href="#transforming-imported-data">Transforming Imported Data</a></li>
<li class="toctree-l2"><a class="reference internal" href="#validating-imported-data">Validating Imported Data</a></li>
<li class="toctree-l2"><a class="reference internal" href="#committing-imported-data">Committing Imported Data</a></li>
<li class="toctree-l2"><a class="reference internal" href="#performance-tips">Performance Tips</a></li>
<li class="toctree-l2"><a class="reference internal" href="#api-endpoints">API Endpoints</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="import_parameters.html">Import Parameters</a></li>
<li class="toctree-l1"><a class="reference internal" href="import_parameters.html#json-jsonl">JSON (JSONL)</a></li>
<li class="toctree-l1"><a class="reference internal" href="vaults_and_objects.html">Vaults and Objects</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Datasets</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="creating_and_migrating_datasets.html">Creating and Migrating Datasets</a></li>
<li class="toctree-l1"><a class="reference internal" href="dataset_templates.html">Dataset Templates</a></li>
<li class="toctree-l1"><a class="reference internal" href="dataset_versioning.html">Dataset Versioning</a></li>
<li class="toctree-l1"><a class="reference internal" href="joining_and_aggregating_datasets.html">Joining and Aggregating Datasets</a></li>
<li class="toctree-l1"><a class="reference internal" href="transforming_datasets.html">Transforming Datasets</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Expressions</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="expressions.html">Expressions</a></li>
<li class="toctree-l1"><a class="reference internal" href="expression_functions.html">Expression Functions</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Global Search</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="data_discovery.html">Data Discovery</a></li>
<li class="toctree-l1"><a class="reference internal" href="metadata_and_global_beacons.html">Metadata and Global Beacons</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Queries and Filters</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="filters.html">Filters</a></li>
<li class="toctree-l1"><a class="reference internal" href="querying_datasets_and_files.html">Querying Datasets and Files</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Changelog</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="changelog.html">Changelog</a></li>
</ul>


<hr />
<ul>
    
    <li class="toctree-l1"><a href="https://pypi.org/project/quartzbio/">Current Version: v1.4.0</a></li>
    
</ul>
<div class="relations">
<h3>Related Topics</h3>
<ul>
  <li><a href="index.html">Documentation overview</a><ul>
      <li>Previous: <a href="exporting_data.html" title="previous chapter">Exporting Data</a></li>
      <li>Next: <a href="import_parameters.html" title="next chapter">Import Parameters</a></li>
  </ul></li>
</ul>
</div>








        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="footer">
      &#169;2025 Precision for Medicine, inc..
      
      |
      Powered by <a href="https://www.sphinx-doc.org/">Sphinx 8.2.3</a>
      &amp; <a href="https://alabaster.readthedocs.io">Alabaster 1.0.0</a>
      
      |
      <a href="_sources/importing_data.md.txt"
          rel="nofollow">Page source</a>
    </div>

    

    
  </body>
</html>